Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/04/19 01:03:06 INFO SparkContext: Running Spark version 1.6.0
16/04/19 01:03:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/04/19 01:03:06 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
16/04/19 01:03:06 WARN Utils: Your hostname, dang-nhuan-optimize-vm resolves to a loopback address: 127.0.0.1; using 192.168.201.29 instead (on interface eth0)
16/04/19 01:03:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/04/19 01:03:06 INFO SecurityManager: Changing view acls to: root
16/04/19 01:03:06 INFO SecurityManager: Changing modify acls to: root
16/04/19 01:03:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
16/04/19 01:03:10 WARN ThreadLocalRandom: Failed to generate a seed from SecureRandom within 3 seconds. Not enough entrophy?
16/04/19 01:03:10 INFO Utils: Successfully started service 'sparkDriver' on port 46621.
16/04/19 01:03:10 INFO Slf4jLogger: Slf4jLogger started
16/04/19 01:03:10 INFO Remoting: Starting remoting
16/04/19 01:03:11 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.201.29:42390]
16/04/19 01:03:11 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 42390.
16/04/19 01:03:11 INFO SparkEnv: Registering MapOutputTracker
16/04/19 01:03:11 INFO SparkEnv: Registering BlockManagerMaster
16/04/19 01:03:11 INFO DiskBlockManager: Created local directory at /home/ubuntu/data/tmp/blockmgr-3367bb98-84da-4a81-a565-6cb08aac05fd
16/04/19 01:03:11 INFO MemoryStore: MemoryStore started with capacity 2.7 GB
16/04/19 01:03:11 INFO SparkEnv: Registering OutputCommitCoordinator
16/04/19 01:03:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/04/19 01:03:11 INFO SparkUI: Started SparkUI at http://192.168.201.29:4040
16/04/19 01:03:11 INFO Utils: Copying /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py to /home/ubuntu/data/tmp/spark-96e017be-8afa-4059-90e5-96dc232a5b0f/userFiles-b4a19c55-0bdc-41f5-81a6-384320f34945/CPUMapping.py
16/04/19 01:03:11 INFO SparkContext: Added file file:/home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py at file:/home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py with timestamp 1461027791803
16/04/19 01:03:11 INFO Executor: Starting executor ID driver on host localhost
16/04/19 01:03:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35626.
16/04/19 01:03:11 INFO NettyBlockTransferService: Server created on 35626
16/04/19 01:03:11 INFO BlockManagerMaster: Trying to register BlockManager
16/04/19 01:03:11 INFO BlockManagerMasterEndpoint: Registering block manager localhost:35626 with 2.7 GB RAM, BlockManagerId(driver, localhost, 35626)
16/04/19 01:03:11 INFO BlockManagerMaster: Registered BlockManager
16/04/19 01:03:39 INFO SparkContext: Starting job: collect at /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py:43
16/04/19 01:03:39 INFO DAGScheduler: Registering RDD 9 (reduceByKey at /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py:39)
16/04/19 01:03:39 INFO DAGScheduler: Got job 0 (collect at /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py:43) with 1 output partitions
16/04/19 01:03:39 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py:43)
16/04/19 01:03:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
16/04/19 01:03:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
16/04/19 01:03:39 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[9] at reduceByKey at /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py:39), which has no missing parents
16/04/19 01:03:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.7 KB, free 13.7 KB)
16/04/19 01:03:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.5 KB, free 21.2 KB)
16/04/19 01:03:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:35626 (size: 7.5 KB, free: 2.7 GB)
16/04/19 01:03:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/04/19 01:03:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[9] at reduceByKey at /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py:39)
16/04/19 01:03:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/04/19 01:03:41 WARN TaskSetManager: Stage 0 contains a task of very large size (324890 KB). The maximum recommended task size is 100 KB.
16/04/19 01:03:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 332687752 bytes)
16/04/19 01:03:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/04/19 01:03:41 INFO Executor: Fetching file:/home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py with timestamp 1461027791803
16/04/19 01:03:41 INFO Utils: /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py has been previously copied to /home/ubuntu/data/tmp/spark-96e017be-8afa-4059-90e5-96dc232a5b0f/userFiles-b4a19c55-0bdc-41f5-81a6-384320f34945/CPUMapping.py
16/04/19 01:03:42 INFO GenerateUnsafeProjection: Code generated in 161.469466 ms
16/04/19 01:03:43 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/worker.py", line 111, in main
    process()
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/worker.py", line 106, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 2346, in pipeline_func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 2346, in pipeline_func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 317, in func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 1776, in combineLocally
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/shuffle.py", line 238, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
TypeError: <lambda>() takes exactly 1 argument (2 given)

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
16/04/19 01:03:43 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/worker.py", line 111, in main
    process()
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/worker.py", line 106, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 2346, in pipeline_func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 2346, in pipeline_func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 317, in func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 1776, in combineLocally
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/shuffle.py", line 238, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
TypeError: <lambda>() takes exactly 1 argument (2 given)

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

16/04/19 01:03:43 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
16/04/19 01:03:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/04/19 01:03:43 INFO TaskSchedulerImpl: Cancelling stage 0
16/04/19 01:03:43 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py:39) failed in 3.627 s
16/04/19 01:03:43 INFO DAGScheduler: Job 0 failed: collect at /home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py:43, took 4.039613 s
Traceback (most recent call last):
  File "/home/ubuntu/NhuanWorking/GoogleClusterExtractor/CPUMapping.py", line 43, in <module>
    t = mapper.collect()
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 771, in collect
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py", line 813, in __call__
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/sql/utils.py", line 45, in deco
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/py4j-0.9-src.zip/py4j/protocol.py", line 308, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/worker.py", line 111, in main
    process()
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/worker.py", line 106, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 2346, in pipeline_func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 2346, in pipeline_func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 317, in func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 1776, in combineLocally
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/shuffle.py", line 238, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
TypeError: <lambda>() takes exactly 1 argument (2 given)

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:209)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/worker.py", line 111, in main
    process()
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/worker.py", line 106, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 2346, in pipeline_func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 2346, in pipeline_func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 317, in func
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/rdd.py", line 1776, in combineLocally
  File "/home/ubuntu/Library/spark-1.6.0/python/lib/pyspark.zip/pyspark/shuffle.py", line 238, in mergeValues
    d[k] = comb(d[k], v) if k in d else creator(v)
TypeError: <lambda>() takes exactly 1 argument (2 given)

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	... 1 more

16/04/19 01:03:43 INFO SparkContext: Invoking stop() from shutdown hook
16/04/19 01:03:44 INFO SparkUI: Stopped Spark web UI at http://192.168.201.29:4040
16/04/19 01:03:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/04/19 01:03:44 INFO MemoryStore: MemoryStore cleared
16/04/19 01:03:44 INFO BlockManager: BlockManager stopped
16/04/19 01:03:44 INFO BlockManagerMaster: BlockManagerMaster stopped
16/04/19 01:03:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/04/19 01:03:44 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/04/19 01:03:44 ERROR Utils: Uncaught exception in thread Thread-5
java.lang.NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet()Ljava/util/concurrent/ConcurrentHashMap$KeySetView;
	at org.apache.spark.rpc.netty.Dispatcher.stop(Dispatcher.scala:175)
	at org.apache.spark.rpc.netty.NettyRpcEnv.cleanup(NettyRpcEnv.scala:299)
	at org.apache.spark.rpc.netty.NettyRpcEnv.shutdown(NettyRpcEnv.scala:271)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1741)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
16/04/19 01:03:44 WARN ShutdownHookManager: ShutdownHook '$anon$2' failed, java.lang.NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet()Ljava/util/concurrent/ConcurrentHashMap$KeySetView;
java.lang.NoSuchMethodError: java.util.concurrent.ConcurrentHashMap.keySet()Ljava/util/concurrent/ConcurrentHashMap$KeySetView;
	at org.apache.spark.rpc.netty.Dispatcher.stop(Dispatcher.scala:175)
	at org.apache.spark.rpc.netty.NettyRpcEnv.cleanup(NettyRpcEnv.scala:299)
	at org.apache.spark.rpc.netty.NettyRpcEnv.shutdown(NettyRpcEnv.scala:271)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1741)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
